{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPI Fraud Detection using Machine Learning\n",
    "\n",
    "This notebook focuses on building a machine learning model to detect fraudulent UPI transactions. We'll follow these steps:\n",
    "\n",
    "1. Data Preprocessing\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Feature Engineering\n",
    "4. Feature Selection\n",
    "5. Model Implementation\n",
    "6. Model Evaluation & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Feature processing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model building and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Other utilities\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../attached_assets/Upi_fraud_dataset-checkpoint.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "for col, count in zip(missing_values.index, missing_values.values):\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count} ({count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate records: {duplicates} ({duplicates/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable (FraudFlag)\n",
    "fraud_distribution = df['FraudFlag'].value_counts(normalize=True) * 100\n",
    "print(\"Distribution of fraud transactions:\")\n",
    "print(fraud_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with median\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill missing categorical values with mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all missing values are handled\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Handle Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate records if any\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Process Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TransactionFrequency numeric value\n",
    "# Example format: '5/day', '3/day'\n",
    "df['TransactionFrequencyValue'] = df['TransactionFrequency'].str.split('/').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns for outlier detection (excluding ID columns and binary flags)\n",
    "numeric_cols_for_outliers = ['Amount', 'Latitude', 'Longitude', 'AvgTransactionAmount', \n",
    "                            'TransactionFrequencyValue', 'FailedAttempts']\n",
    "\n",
    "# Check for outliers using box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols_for_outliers):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cap outliers using IQR method\n",
    "def cap_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier capping to numeric columns\n",
    "for col in numeric_cols_for_outliers:\n",
    "    df = cap_outliers(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outlier treatment\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols_for_outliers):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col} (After Treatment)')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Distribution of Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of fraud vs non-fraud transactions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='FraudFlag', data=df)\n",
    "plt.title('Distribution of Fraud vs Non-Fraud Transactions')\n",
    "plt.xlabel('Fraud Flag (1 = Fraud, 0 = Non-Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add percentage labels\n",
    "total = len(df)\n",
    "for p in plt.gca().patches:\n",
    "    percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "    plt.gca().annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                 ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transaction Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of transaction amounts\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Amount'], kde=True)\n",
    "plt.title('Distribution of Transaction Amounts')\n",
    "plt.xlabel('Amount')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='FraudFlag', y='Amount', data=df)\n",
    "plt.title('Transaction Amount by Fraud Status')\n",
    "plt.xlabel('Fraud Flag (1 = Fraud, 0 = Non-Fraud)')\n",
    "plt.ylabel('Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average transaction amounts for fraud vs non-fraud\n",
    "fraud_avg = df[df['FraudFlag'] == True]['Amount'].mean()\n",
    "non_fraud_avg = df[df['FraudFlag'] == False]['Amount'].mean()\n",
    "\n",
    "print(f\"Average amount for fraud transactions: ${fraud_avg:.2f}\")\n",
    "print(f\"Average amount for non-fraud transactions: ${non_fraud_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time components\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "df['Month'] = df['Timestamp'].dt.month\n",
    "df['Year'] = df['Timestamp'].dt.year\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['DayOfWeek'] = df['Timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud by hour of day\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "hourly_fraud = df.groupby(['Hour', 'FraudFlag']).size().unstack().fillna(0)\n",
    "hourly_fraud_rate = (hourly_fraud[True] / (hourly_fraud[True] + hourly_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Hour', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "hourly_fraud_rate.plot(kind='line', marker='o')\n",
    "plt.title('Fraud Rate by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud by day of week\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df['DayName'] = df['DayOfWeek'].map(lambda x: days[x])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "daily_fraud = df.groupby(['DayName', 'FraudFlag']).size().unstack().fillna(0)\n",
    "daily_fraud = daily_fraud.reindex(days)  # Reorder by day of week\n",
    "daily_fraud_rate = (daily_fraud[True] / (daily_fraud[True] + daily_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "day_order = pd.CategoricalDtype(categories=days, ordered=True)\n",
    "df['DayName'] = df['DayName'].astype(day_order)\n",
    "sns.countplot(x='DayName', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "daily_fraud_rate.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Transaction Type and Merchant Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud by transaction type\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "tx_type_fraud = df.groupby(['TransactionType', 'FraudFlag']).size().unstack().fillna(0)\n",
    "tx_type_fraud_rate = (tx_type_fraud[True] / (tx_type_fraud[True] + tx_type_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='TransactionType', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Type')\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "tx_type_fraud_rate.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Transaction Type')\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud by merchant category\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "merchant_fraud = df.groupby(['MerchantCategory', 'FraudFlag']).size().unstack().fillna(0)\n",
    "merchant_fraud_rate = (merchant_fraud[True] / (merchant_fraud[True] + merchant_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(y='MerchantCategory', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Merchant Category')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Merchant Category')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "merchant_fraud_rate.sort_values().plot(kind='barh')\n",
    "plt.title('Fraud Rate by Merchant Category')\n",
    "plt.xlabel('Fraud Rate (%)')\n",
    "plt.ylabel('Merchant Category')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Location Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting fraudulent vs. non-fraudulent transactions on a scatter plot based on location\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Sample the data if it's too large for visualization\n",
    "sample_size = min(5000, len(df))\n",
    "df_sample = df.sample(sample_size, random_state=42)\n",
    "\n",
    "# Create scatter plot with different colors for fraud vs non-fraud\n",
    "fraud = df_sample[df_sample['FraudFlag'] == True]\n",
    "non_fraud = df_sample[df_sample['FraudFlag'] == False]\n",
    "\n",
    "plt.scatter(non_fraud['Longitude'], non_fraud['Latitude'], \n",
    "            alpha=0.6, c='blue', s=15, label='Non-Fraud')\n",
    "plt.scatter(fraud['Longitude'], fraud['Latitude'], \n",
    "            alpha=0.6, c='red', s=30, label='Fraud')\n",
    "\n",
    "plt.title('Geographical Distribution of Transactions')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud for unusual locations\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "unusual_loc_fraud = df.groupby(['UnusualLocation', 'FraudFlag']).size().unstack().fillna(0)\n",
    "unusual_loc_fraud_rate = (unusual_loc_fraud[True] / (unusual_loc_fraud[True] + unusual_loc_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='UnusualLocation', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Location Status')\n",
    "plt.xlabel('Unusual Location')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "unusual_loc_fraud_rate.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Location Status')\n",
    "plt.xlabel('Unusual Location')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Device and IP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud for new devices\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "new_device_fraud = df.groupby(['NewDevice', 'FraudFlag']).size().unstack().fillna(0)\n",
    "new_device_fraud_rate = (new_device_fraud[True] / (new_device_fraud[True] + new_device_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='NewDevice', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Device Status')\n",
    "plt.xlabel('New Device')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "new_device_fraud_rate.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Device Status')\n",
    "plt.xlabel('New Device')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean to numeric for correlation analysis\n",
    "df_corr = df.copy()\n",
    "bool_cols = ['UnusualLocation', 'UnusualAmount', 'NewDevice', 'FraudFlag']\n",
    "for col in bool_cols:\n",
    "    df_corr[col] = df_corr[col].astype(int)\n",
    "\n",
    "# Select relevant numeric columns for correlation analysis\n",
    "corr_cols = ['Amount', 'AvgTransactionAmount', 'TransactionFrequencyValue', \n",
    "             'UnusualLocation', 'UnusualAmount', 'NewDevice', 'FailedAttempts', 'FraudFlag',\n",
    "             'Hour', 'DayOfWeek']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df_corr[corr_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Failed Attempts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze failed attempts relation to fraud\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Count plot of failed attempts by fraud status\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='FailedAttempts', hue='FraudFlag', data=df)\n",
    "plt.title('Failed Attempts by Fraud Status')\n",
    "plt.xlabel('Number of Failed Attempts')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "# Calculate fraud rate for each number of failed attempts\n",
    "failed_attempts_fraud = df.groupby(['FailedAttempts', 'FraudFlag']).size().unstack().fillna(0)\n",
    "failed_attempts_fraud_rate = (failed_attempts_fraud[True] / (failed_attempts_fraud[True] + failed_attempts_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "failed_attempts_fraud_rate.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Number of Failed Attempts')\n",
    "plt.xlabel('Number of Failed Attempts')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Bank Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud by bank\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "bank_fraud = df.groupby(['BankName', 'FraudFlag']).size().unstack().fillna(0)\n",
    "bank_fraud_rate = (bank_fraud[True] / (bank_fraud[True] + bank_fraud[False])) * 100\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(y='BankName', hue='FraudFlag', data=df)\n",
    "plt.title('Transactions by Bank')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Bank Name')\n",
    "plt.legend(title='Fraud Flag')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bank_fraud_rate.sort_values().plot(kind='barh')\n",
    "plt.title('Fraud Rate by Bank')\n",
    "plt.xlabel('Fraud Rate (%)')\n",
    "plt.ylabel('Bank Name')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weekend flag\n",
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)  # 5 = Saturday, 6 = Sunday\n",
    "\n",
    "# Create night time flag (10 PM - 6 AM)\n",
    "df['IsNightTime'] = df['Hour'].apply(lambda x: 1 if (x >= 22 or x < 6) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Amount-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create amount ratio feature\n",
    "df['AmountRatio'] = df['Amount'] / df['AvgTransactionAmount']\n",
    "\n",
    "# Create amount difference feature\n",
    "df['AmountDiff'] = df['Amount'] - df['AvgTransactionAmount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Risk Score Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a risk score combining multiple risk factors\n",
    "df['RiskScore'] = df['UnusualLocation'].astype(int) + \\\n",
    "                 df['UnusualAmount'].astype(int) + \\\n",
    "                 df['NewDevice'].astype(int) + \\\n",
    "                 df['FailedAttempts'] + \\\n",
    "                 df['IsNightTime']\n",
    "\n",
    "# Analyze risk score effectiveness\n",
    "plt.figure(figsize=(12, 6))\n",
    "risk_score_fraud = df.groupby(['RiskScore', 'FraudFlag']).size().unstack().fillna(0)\n",
    "if True in risk_score_fraud.columns:  # Check if there are fraud cases in the score ranges\n",
    "    risk_score_fraud_rate = (risk_score_fraud[True] / (risk_score_fraud[True] + risk_score_fraud[False])) * 100\n",
    "\n",
    "    plt.bar(risk_score_fraud_rate.index, risk_score_fraud_rate.values)\n",
    "    plt.title('Fraud Rate by Risk Score')\n",
    "    plt.xlabel('Risk Score')\n",
    "    plt.ylabel('Fraud Rate (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Phone Number Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean phone numbers\n",
    "df['PhoneNumber'] = df['PhoneNumber'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "# Extract country code (assuming +91 for India)\n",
    "df['HasCountryCode'] = df['PhoneNumber'].apply(lambda x: 1 if x.startswith('91') else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 IP Address Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IP address octets\n",
    "df['IP_FirstOctet'] = df['IPAddress'].str.split('.').str[0].astype(int)\n",
    "\n",
    "# Create High-Risk IP Flag (simplified approach)\n",
    "# IPs in certain ranges are often used for VPNs or are high-risk\n",
    "# This is a simplified approach - in production, you would use IP reputation databases\n",
    "high_risk_ranges = [(0, 10), (172, 172), (192, 192), (198, 198)]\n",
    "df['HighRiskIP'] = df['IP_FirstOctet'].apply(\n",
    "    lambda x: 1 if any(lower <= x <= upper for lower, upper in high_risk_ranges) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Data for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# Exclude non-predictive columns\n",
    "exclude_cols = ['TransactionID', 'UserID', 'DeviceID', 'IPAddress', 'PhoneNumber', \n",
    "                'Timestamp', 'TransactionFrequency', 'DayName']\n",
    "\n",
    "# Define categorical columns for encoding\n",
    "categorical_cols = ['MerchantCategory', 'TransactionType', 'BankName']\n",
    "\n",
    "# Define numerical columns to be scaled\n",
    "numerical_cols = ['Amount', 'Latitude', 'Longitude', 'AvgTransactionAmount', \n",
    "                  'TransactionFrequencyValue', 'FailedAttempts', 'Day', 'Month', 'Year', \n",
    "                  'Hour', 'DayOfWeek', 'AmountRatio', 'AmountDiff', 'RiskScore', 'IP_FirstOctet']\n",
    "\n",
    "# Define boolean columns (already 0/1)\n",
    "boolean_cols = ['UnusualLocation', 'UnusualAmount', 'NewDevice', 'IsWeekend', \n",
    "                'IsNightTime', 'HasCountryCode', 'HighRiskIP']\n",
    "\n",
    "# All features\n",
    "all_features = categorical_cols + numerical_cols + boolean_cols\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "X = df[all_features]\n",
    "y = df['FraudFlag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preprocess Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical features with one-hot encoding\n",
    "ohe = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\n",
    "cat_features_train = ohe.fit_transform(X_train[categorical_cols])\n",
    "cat_features_test = ohe.transform(X_test[categorical_cols])\n",
    "\n",
    "# Create DataFrame with encoded features\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "cat_features_train_df = pd.DataFrame(cat_features_train, columns=cat_feature_names, index=X_train.index)\n",
    "cat_features_test_df = pd.DataFrame(cat_features_test, columns=cat_feature_names, index=X_test.index)\n",
    "\n",
    "# Preprocess numerical features with standard scaling\n",
    "scaler = StandardScaler()\n",
    "num_features_train = scaler.fit_transform(X_train[numerical_cols])\n",
    "num_features_test = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Create DataFrame with scaled features\n",
    "num_features_train_df = pd.DataFrame(num_features_train, columns=numerical_cols, index=X_train.index)\n",
    "num_features_test_df = pd.DataFrame(num_features_test, columns=numerical_cols, index=X_test.index)\n",
    "\n",
    "# Combine all features\n",
    "X_train_processed = pd.concat([num_features_train_df, cat_features_train_df, X_train[boolean_cols]], axis=1)\n",
    "X_test_processed = pd.concat([num_features_test_df, cat_features_test_df, X_test[boolean_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Feature importance from Random Forest\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train_processed.columns,\n",
    "    'Importance': rf_selector.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "plt.title('Top 20 Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Select features using RFE with Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe_selector = RFE(estimator=lr, n_features_to_select=20, step=1)\n",
    "rfe_selector.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_rfe = X_train_processed.columns[rfe_selector.support_]\n",
    "print(\"Features selected by RFE:\")\n",
    "print(selected_features_rfe.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Select features using XGBoost feature importance\n",
    "xgb_selector = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_selector.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "xgb_importances = pd.DataFrame({\n",
    "    'Feature': X_train_processed.columns,\n",
    "    'Importance': xgb_selector.feature_importances_\n",
    "})\n",
    "xgb_importances = xgb_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=xgb_importances.head(20))\n",
    "plt.title('Top 20 Features by XGBoost Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Select Final Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from different feature selection methods\n",
    "top_rf_features = feature_importances.head(20)['Feature'].tolist()\n",
    "top_xgb_features = xgb_importances.head(20)['Feature'].tolist()\n",
    "\n",
    "# Find common features across methods\n",
    "common_features = list(set(top_rf_features) & set(top_xgb_features) & set(selected_features_rfe))\n",
    "print(f\"Number of common features across all methods: {len(common_features)}\")\n",
    "\n",
    "# If not enough common features, take union of top features from each method\n",
    "if len(common_features) < 10:\n",
    "    # Take top 15 from each method\n",
    "    top_rf_features = feature_importances.head(15)['Feature'].tolist()\n",
    "    top_xgb_features = xgb_importances.head(15)['Feature'].tolist()\n",
    "    selected_features_rfe_top = list(selected_features_rfe)[:15] if len(selected_features_rfe) > 15 else selected_features_rfe\n",
    "    \n",
    "    # Combine all unique features\n",
    "    final_features = list(set(top_rf_features + top_xgb_features + selected_features_rfe_top))\n",
    "else:\n",
    "    final_features = common_features\n",
    "\n",
    "print(f\"Final number of selected features: {len(final_features)}\")\n",
    "print(\"Selected features:\")\n",
    "print(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final datasets with selected features\n",
    "X_train_final = X_train_processed[final_features]\n",
    "X_test_final = X_test_processed[final_features]\n",
    "\n",
    "print(f\"Shape of final training dataset: {X_train_final.shape}\")\n",
    "print(f\"Shape of final test dataset: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = np.mean(y_train_pred == y_train)\n",
    "    test_accuracy = np.mean(y_test_pred == y_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'],\n",
    "                yticklabels=['Non-Fraud', 'Fraud'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve if applicable\n",
    "    if y_test_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, lw=2)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_test_prob': y_test_prob\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Model:\\n\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "lr_results = evaluate_model(lr_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Model:\\n\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "dt_results = evaluate_model(dt_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Model:\\n\")\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced')\n",
    "rf_results = evaluate_model(rf_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine Model:\\n\")\n",
    "svm_model = SVC(random_state=42, probability=True, class_weight='balanced')\n",
    "svm_results = evaluate_model(svm_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGBoost Model:\\n\")\n",
    "# Calculate scale_pos_weight for imbalanced dataset\n",
    "scale_pos_weight = len(y_train[y_train == False]) / len(y_train[y_train == True])\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, n_estimators=100, scale_pos_weight=scale_pos_weight)\n",
    "xgb_results = evaluate_model(xgb_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune the hyperparameters for the XGBoost model\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Using RandomizedSearchCV to speed up the process\n",
    "xgb_tuned = xgb.XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight)\n",
    "random_search = RandomizedSearchCV(xgb_tuned, param_distributions=param_grid, n_iter=10, \n",
    "                                  scoring='f1', cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with best parameters\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "print(\"XGBoost Model with Tuned Hyperparameters:\\n\")\n",
    "xgb_tuned_results = evaluate_model(best_xgb_model, X_train_final, X_test_final, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "models = [\n",
    "    {'name': 'Logistic Regression', 'results': lr_results},\n",
    "    {'name': 'Decision Tree', 'results': dt_results},\n",
    "    {'name': 'Random Forest', 'results': rf_results},\n",
    "    {'name': 'SVM', 'results': svm_results},\n",
    "    {'name': 'XGBoost', 'results': xgb_results},\n",
    "    {'name': 'XGBoost (Tuned)', 'results': xgb_tuned_results}\n",
    "]\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each model\n",
    "for model_info in models:\n",
    "    results = model_info['results']\n",
    "    y_pred = results['y_test_pred']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate ROC-AUC if probabilities are available\n",
    "    if results['y_test_prob'] is not None:\n",
    "        roc_auc = roc_auc_score(y_test, results['y_test_prob'])\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Store metrics in results dictionary\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1_score'] = f1\n",
    "    results['roc_auc'] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'ROC-AUC': []\n",
    "}\n",
    "\n",
    "for model_info in models:\n",
    "    comparison_data['Model'].append(model_info['name'])\n",
    "    comparison_data['Accuracy'].append(model_info['results']['test_accuracy'])\n",
    "    comparison_data['Precision'].append(model_info['results']['precision'])\n",
    "    comparison_data['Recall'].append(model_info['results']['recall'])\n",
    "    comparison_data['F1 Score'].append(model_info['results']['f1_score'])\n",
    "    comparison_data['ROC-AUC'].append(model_info['results']['roc_auc'])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1 Score', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Prepare data for plotting\n",
    "models_for_plot = comparison_df['Model']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "metrics_data = comparison_df[metrics].values\n",
    "\n",
    "# Plot each metric\n",
    "x = np.arange(len(models_for_plot))\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "for attribute, measurement in zip(metrics, metrics_data.T):\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(models_for_plot, rotation=45, ha='right')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on F1 score\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_index = [model_info['name'] for model_info in models].index(best_model_name)\n",
    "best_model = models[best_model_index]['results']['model']\n",
    "\n",
    "print(f\"The best model is: {best_model_name}\")\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'ohe': ohe,\n",
    "    'scaler': scaler,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'boolean_cols': boolean_cols,\n",
    "    'final_features': final_features\n",
    "}\n",
    "\n",
    "# Save model and preprocessing objects to disk\n",
    "import pickle\n",
    "\n",
    "# Save best model\n",
    "with open('../models/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "    \n",
    "# Save preprocessing objects\n",
    "with open('../models/preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(\"Model and preprocessing objects saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we built a comprehensive fraud detection system for UPI transactions. We've:\n",
    "\n",
    "1. Preprocessed the data by handling missing values, duplicates, and outliers\n",
    "2. Performed exploratory data analysis to understand patterns in fraudulent transactions\n",
    "3. Created new features to improve model performance\n",
    "4. Selected the most important features using various methods\n",
    "5. Implemented and evaluated multiple classification models\n",
    "6. Tuned hyperparameters to improve performance\n",
    "7. Selected the best model based on performance metrics\n",
    "\n",
    "The best performing model can now be deployed to a web application using Streamlit, which is implemented separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
